
 âœ… 1. Test Strategy vs Test Approach

 â­ A. Test Strategy (HIGH LEVEL â€” What & Why)

Analogy:
Imagine planning a trip.

 Test Strategy = deciding the destination, budget, and purpose of the trip.

You do NOT worry about:

 Which taxi to use
 What time to leave

Those are details (Test Approach).

 âœ” Technical

Test Strategy specifies:

 Overall testing goals
 What types of testing will be done
  (Functional, Regression, Performance, Security, Compatibilityâ€¦)
 Testing levels
  (Unit â†’ Integration â†’ System â†’ UAT)
 Testing tools
  (Selenium, JMeter, Postman)
 Risk-based testing decisions

It is usually created by QA Manager for the entire organization/project.

---

 â­ B. Test Approach (LOW LEVEL â€” How & When)

Analogy:
If Test Strategy is â€œwe will travel to Goa by trainâ€,
Test Approach is the details:

 Which train?
 What time?
 Which platform?
 What is the packing plan?
 Who will go where?

 âœ” Technical

Test Approach includes details of:

 How test cases will be written
 How test data will be prepared
 How defects will be logged
 How regression tests will be executed
 How environments will be used
 How risks will be mitigated
 Step-by-step execution plan

It is more practical and tactical.

---

 ğŸŸ© Summary Table â€” Strategy vs Approach

| Topic      | Test Strategy    | Test Approach             |
| ---------- | ---------------- | ------------------------- |
| Level      | High-level       | Low-level                 |
| Focus      | What & Why       | How & When                |
| Created by | QA Manager       | Test Lead                 |
| Example    | Types of testing | Steps for executing tests |

---

 âœ… 2. Entry Criteria & Exit Criteria

Also called Definition of Ready (DoR) and Definition of Done (DoD).

---

 â­ A. Entry Criteria = Definition of Ready

Meaning:
Conditions that must be fulfilled before testing can start.

Analogy:
Before cooking food:

 Vegetables must be washed
 Stove must be ready
 Ingredients available

Without this â†’ cooking will fail.

 âœ” Technical

Entry criteria include:

 Requirements document available (SRS/CRS)
 Test data ready
 Test environment up & running
 Build deployed successfully
 Necessary tools/configurations ready

If entry criteria are not met â†’ testing starts late â†’ poor quality.

---

 â­ B. Exit Criteria = Definition of Done

Meaning:
Conditions that must be met before testing can stop.

Analogy:
Before you serve food:

 Food fully cooked
 Taste checked
 Gas turned off
 Kitchen cleaned

 âœ” Technical

Exit criteria include:

 All test cases executed
 All high/critical bugs fixed
 No open severity-1 or severity-2 bugs
 Test coverage achieved (ex: 90%)
 Regression testing completed

If exit criteria are not met â†’ product is not ready for release.

---

 ğŸŸ© Summary Table â€” Entry vs Exit Criteria

| Entry Criteria (DoR) | Exit Criteria (DoD)     |
| -------------------- | ----------------------- |
| Testing can start    | Testing can stop        |
| Requirements ready   | All test cases executed |
| Environment ready    | All critical bugs fixed |

---

 âœ… 3. Test Execution Schedule

This explains WHEN each testing activity will happen.

Analogy:
A school timetable.

 Maths: 9â€“10
 Science: 10â€“11
 English: 11â€“12

Same way, testing has a timetable.

 âœ” Technical Example

A typical test execution schedule:

| Week   | Activity                             |
| ------ | ------------------------------------ |
| Week 1 | Test planning + test case writing    |
| Week 2 | Test case review + environment setup |
| Week 3 | Test execution                       |
| Week 4 | Bug fixing                           |


The schedule helps manage:

 Resources
 Environment availability
 Build delivery
 Deadlines

---

 âœ… 4. Factors Influencing Test Effort

Why does testing take long or short?
Many factors affect it.

---

 â­ Analogy

Suppose you're doing house cleaning.

Effort depends on:

 Size of the house
 How dirty it is
 How many people are helping
 Number of tools (vacuum cleaner, broom)
 Your experience in cleaning

Similarly, testing effort depends on many factors.


 â­ Factors Influencing Test Effort (Technical + Easy Analogy)

These are the things that decide how much time, effort, and resources testing will need.

Think of testing like preparing a big meal.
Different dishes require different:

 time
 ingredients
 chefs
 tools

Same with testing â€” different features require different effort.

Letâ€™s explain each factor with super easy English + analogy + technical meaning.

---

 âœ… 1ï¸âƒ£ Complexity of Requirements

 âœ” Easy English

If the feature is big or complicated, it needs more testing time.

 âœ” Analogy

Making biryani takes more time than making maggi.
More steps â†’ more time.

 âœ” Technical Meaning

A complex feature like:

 Payment gateway
 Multi-step checkout
 AI recommendations
  needs more:
 test cases
 scenarios
 negative tests
  â†’ so effort increases.

---

 âœ… 2ï¸âƒ£ Quality of Requirements Document (SRS/CRS)

 âœ” Easy English

If requirements are not clear, testers waste time asking questions.

 âœ” Analogy

If someone gives you a recipe with half instructions missingâ€¦
you will keep asking: â€œWhat next?â€
This wastes time.

 âœ” Technical Meaning

Poor SRS =

 misunderstandings
 rework
 wrong test cases
 more clarification meetings
  â†’ testing effort increases.

---

 âœ… 3ï¸âƒ£ Number of Test Cases

 âœ” Easy English

More test cases = more time to execute.

 âœ” Analogy

If you wash 20 clothes, it takes less time;
If you wash 200 clothes, it takes more time.

 âœ” Technical Meaning

Each test case requires:

 execution
 result validation
 defect logging (if fails)

So number of test cases has direct impact on time.

---

 âœ… 4ï¸âƒ£ Skill Level of Testers

 âœ” Easy English

Experienced testers finish faster and catch more bugs.

 âœ” Analogy

A trained chef cooks quickly and perfectly.
A new cook takes more time and makes mistakes.

 âœ” Technical Meaning

Senior testers:

 write better test cases
 execute faster
 understand requirements quickly
  â†’ testing effort reduces.

Junior testers:

 need guidance
 take more time
 may miss scenarios
  â†’ effort increases.

---

 âœ… 5ï¸âƒ£ Tools Used (Manual vs Automation)

 âœ” Easy English

Automation tools help finish work faster.

 âœ” Analogy

A washing machine washes faster than doing it by hand.

 âœ” Technical Meaning

Automation (Selenium, Playwright):

 fast execution
 repeatable
 good for regression

Manual testing:

 slower
 more human effort

Choosing the right tools affects effort.

---

 âœ… 6ï¸âƒ£ Defect Density (Number of Bugs)

 âœ” Easy English

More bugs = more retesting = more effort.

 âœ” Analogy

If the food you cooked tastes bad, you must correct it â†’ more time.

 âœ” Technical Meaning

High defect density means:

 more retesting
 more communication with developers

This increases testing time.

---

 âœ… 7ï¸âƒ£ Test Environment Stability

 âœ” Easy English

If the test server keeps crashing, testers cannot test.

 âœ” Analogy

If there is no electricity, you cannot cook or iron clothes.
Work stops.

 âœ” Technical Meaning

Unstable environment causes:

 test delays
 partial execution
 false failures
 repeated testing

â†’ effort increases a lot.

---

 âœ… 8ï¸âƒ£ Dependency on Developers / Other Teams

 âœ” Easy English

If developers delay delivering the build, testing also gets delayed.

 âœ” Analogy

You canâ€™t start making tea if milk is not delivered yet.

 âœ” Technical Meaning

Testing depends on:

 build availability
 database setup
 third-party APIs

Any delay increases test effort and shifts the schedule.

---

 âœ… 9ï¸âƒ£ Integration With Third-Party Systems

 âœ” Easy English

If your system depends on other systems, their issues delay your testing.

 âœ” Analogy

If you depend on Swiggy delivery and the rider is late â†’ your entire plan gets delayed.

 âœ” Technical Meaning

Example systems:

 Paytm API
 Google Maps API
 OTP gateways

If they:

 go down
 timeout
 respond slowly

then testing stops â†’ effort increases.

---

 âœ… ğŸ”Ÿ Risk Level of the Module

 âœ” Easy English

High-risk features need extra testing.

 âœ” Analogy

When you cook for guests, you double-check everything more carefully.
But when cooking for yourself, you donâ€™t.

 âœ” Technical Meaning

High-risk modules like:

 Banking transactions
 Payments
 Data security
 Healthcare

need:

 more test cases
 deeper testing
 multiple cycles

So effort is higher.

---

 â­ 1. Test Monitoring and Control

 âœ” What is Test Monitoring?

Monitoring = Checking the progress of testing every day.

 âœ” Analogy (Easy)

Imagine youâ€™re cooking biryani.

 You keep checking:
  âœ Is rice cooked?
  âœ Is salt enough?
  âœ Is chicken soft?

You monitor the dish so it doesnâ€™t burn or go wrong.

 âœ” Technical Meaning

Monitoring means:

 Tracking test progress
 Tracking test coverage
 Tracking number of defects found vs fixed
 Tracking schedule delays

Tools used: Jira, TestRail, Zephyr, Excel reports

---

 âœ” What is Test Control?

Control = Taking action when something goes wrong.

 âœ” Analogy

Cooking biryani â†’ if rice is undercooked:
You take action:

 Add more water
 Extend cooking time

Same in testing.

 âœ” Technical Meaning

Test Manager performs actions like:

 Adding more testers if delays happen
 Extending deadlines
 Increasing test time for risky modules
 Changing test priorities
 Asking developers to fix high-priority bugs first

---

 â­ 2. Metrics Used in Testing

Metric = A number that shows testing progress or quality.

 âœ” Analogy

In cricket, you track:

 Run rate
 Wickets
 Strike rate
 Economy

Same way, in testing we track numerical indicators.

---

 âœ” Important Testing Metrics (Easy + Technical)

 1ï¸âƒ£ Test Case Execution Metrics

Shows how many test cases are executed.

 Total test cases = 100
 Executed = 60
 Passed = 50
 Failed = 10
 Not executed = 40

Analogy:
Like checking how many chapters you studied out of total.

---

 2ï¸âƒ£ Defect Metrics

Shows defect health.

 Total defects
 Open defects
 Closed defects
 Severity & priority
 Defect density (bugs per module)

Analogy:
If you clean your room and find 30 dust spots in one corner, that corner is â€œhigh defect density.â€

---

 4ï¸âƒ£ Test Productivity Metrics

= Test cases executed per day per tester.

Analogy:
Number of pages you read per hour while studying.

---

 5ï¸âƒ£ Defect Removal Efficiency (DRE)

How many bugs were caught by testers (not customers)?

Formula:
DRE = Bugs found in testing / Total bugs

Analogy:
Checking how many mistakes YOU catch before teacher checks your notebook.

---

 6ï¸âƒ£ Automation Metrics

 % of test cases automated
 Automation execution time

Analogy:
How many household tasks you automated with a washing machine instead of doing manually.

---

 â­ 3. Test Reports (Purpose, Contents, Audience)

 âœ” What is a Test Report?

It is a summary of the testing activities, created at the end of the test cycle.

 âœ” Analogy

Like your mark sheet after exams:

 Marks
 Strengths
 Weak areas
 Final result

Same concept.

---

 â­ A. Purpose of Test Reports

Why do we need test reports?

1. Show overall quality of the software
2. Help management decide whether to release or delay the product
3. Provide status of testing progress
4. Highlight risks
5. Communicate defects status
6. Provide traceability with requirements

---

 â­ B. Contents of a Test Report (Easy + Technical)

| Section                   | Meaning (Easy English)             | Analogy                                            |
| ------------------------- | ---------------------------------- | -------------------------------------------------- |
| Test Summary          | What we tested                     | Overview like â€œExam was Maths, Physics, Chemistryâ€ |
| Test Execution Status | Passed, failed, blocked test cases | Your marks: passed/failed subjects                 |
| Defect Summary        | Bugs found grouped by severity     | Mistakes found in your notebook                    |
| Environment Details   | Browser, OS, devices used          | Exam hall details                                  |
| Risks & Issues        | Problems during testing            | â€œFan not working during examâ€                      |
| Recommendations       | Ready to release or not            | Teacher saying â€œyou can pass to next classâ€        |
| Test Coverage         | How much requirement tested        | â€œCompleted 90% of syllabusâ€                        |

---

 â­ C. Audience of Test Reports

Who reads test reports?

 1ï¸âƒ£ Project Manager

 Checks progress
 Decides release date

 2ï¸âƒ£ Client / Product Owner

 Understands product quality
 Takes business decisions

 3ï¸âƒ£ Developers

 Check defects
 Identify impacted modules

 4ï¸âƒ£ QA Team / Testers

 Review test effectiveness

 5ï¸âƒ£ Senior Management

 For future planning & budgeting

 â­ 1. Risks and Testing

 â— Simple Meaning

Risk = Something bad may happen in the future.

Testing deals with risks by:

 Identifying risks
 Planning tests to reduce those risks
 Tracking risks until project ends

 âœ” Analogy

Before crossing the road, you check left-right.
Why?
Because there is a risk of accident.

Testing also checks software from all sides to avoid "accidents" in production.

 âœ” Technical Meaning

Risk in testing means:

> â€œA problem that MIGHT happen if software has defects.â€

Example risks a tester considers:

 Login might fail
 Payment may be deducted twice
 System may crash with 1000 users
 Wrong tax calculation

Testing reduces the chances of these problems.

---

 â­ 2. Definition of Risk

 âœ” Easy Definition

Risk = Probability of something going wrong Ã— Impact if it goes wrong

 âœ” Analogy

Rain Risk Example:

 Probability = 60%
 Impact = youâ€™ll get wet

So you carry an umbrella.

 âœ” Technical Example

Risk in an e-commerce app:

 Probability: Payment gateway might fail under high load
 Impact: Customers lose money â†’ business loss

So testers perform load testing with JMeter.

---

 â­ 3. Product Risk

 âœ” Simple Meaning

Risk related to the software itself.

 âœ” Analogy

If a car has a braking issue â†’ it is a product risk.

 âœ” Technical Examples

Product risks include:

1. Incorrect login functionality
2. Payment gateway failure
3. App crashes
4. Security vulnerabilities
5. Slow performance
6. Wrong business calculations
7. Data not saved correctly

These risks come from the software quality.

 âœ” Testing Focus

 Functional testing
 Regression testing
 Performance testing
 Security testing

Product risks â‰ˆ â€œWhat can go wrong inside the product?â€

---

 â­ 4. Project Risk

 âœ” Simple Meaning

Risks related to project execution, not the software.

 âœ” Analogy

If the driver leaves late â†’ the car is fine, but you might reach late.

Same for projects â€” nothing wrong with the software yet, but the project may delay.

 âœ” Technical Examples

Project risks include:

1. Requirement delays

    Clients take too long to give SRS

2. Developer availability

    Key developer resigns

3. Test environment not ready

    Server not deployed
    Database down

4. Budget issues

    No money for automation tools

5. Unrealistic deadlines

    Tester is given 4 days for 10 days of testing

6. Build instability

    Frequent crashes delay QA

These risks impact schedule, cost, and team.

 âœ” Testing Focus

 Test planning adjustments
 Re-estimating effort
 Communicating delays
 Risk-based testing (test most risky things first)

Project risks â‰ˆ â€œWhat can go wrong in the project execution?â€

---

